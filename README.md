
# ETL Project

## Project Structure

```
etl_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_data.csv
â”‚   â”œâ”€â”€ sample_data.json
â”œâ”€â”€ etl_pipeline.py
â”œâ”€â”€ etl_log.txt
â”œâ”€â”€ etl_config.json
â”œâ”€â”€ database.db
â””â”€â”€ README.md
```

## Overview

This project implements a simple ETL (Extract, Transform, Load) pipeline in Python.  
The pipeline extracts data from multiple sources (CSV, JSON, API), transforms it into a standardized format, and loads it into an SQLite database.

## Files Description

- **data/sample_data.csv**: Sample CSV data file.
- **data/sample_data.json**: Sample JSON data file.
- **etl_pipeline.py**: Main Python script containing the ETL logic.
- **etl_log.txt**: Log file where ETL process messages are appended.
- **etl_config.json**: (Optional) Configuration file for ETL parameters (not used in current code).
- **database.db**: SQLite database file generated by the ETL process.
- **README.md**: This documentation file.

## Detailed Explanation of `etl_pipeline.py`

### 1. Database Setup

We use SQLAlchemy to create and connect to a SQLite database named `database.db`.  
A table called `users` is defined with columns:
- `id`: Primary key, autoincrement integer.
- `name`: String
- `age`: Integer (nullable)
- `city`: String
- `source`: String (indicates the origin of the data: CSV, JSON, or API)

```python
from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table

engine = create_engine('sqlite:///database.db')
metadata = MetaData()

users = Table('users', metadata,
              Column('id', Integer, primary_key=True, autoincrement=True),
              Column('name', String),
              Column('age', Integer, nullable=True),
              Column('city', String),
              Column('source', String)
              )

metadata.create_all(engine)
```

### 2. Logging

The function `log_message` writes messages with timestamps to `etl_log.txt` for monitoring ETL progress and errors.

```python
from datetime import datetime

def log_message(message):
    with open("etl_log.txt", "a") as log_file:
        log_file.write(f"{datetime.now()}: {message}\n")
```

### 3. Extraction Functions

- `extract_csv(file_path)`: Reads CSV file into a DataFrame.
- `extract_json(file_path)`: Reads JSON file into a DataFrame.
- `extract_api(url)`: Fetches JSON data from a REST API and normalizes it into a DataFrame.

Each extraction logs success or error messages.

### 4. Transformation

The function `transform_data(df, source)` standardizes the column names and adds a `source` column to indicate the data origin. It handles each data type separately.

### 5. Loading

The function `load_data(df)` appends the transformed data into the `users` table in the SQLite database.

### 6. ETL Execution

The `run_etl()` function orchestrates the whole process:

- Extract data from CSV, JSON, and API.
- Transform each dataset.
- Load the transformed data into the database.
- Log start and completion of the ETL job.

```python
def run_etl():
    log_message("ETL Job Started.")

    csv_data = extract_csv('data/sample_data.csv')
    json_data = extract_json('data/sample_data.json')
    api_data = extract_api('https://jsonplaceholder.typicode.com/users')

    csv_data = transform_data(csv_data, 'csv')
    json_data = transform_data(json_data, 'json')
    api_data = transform_data(api_data, 'api')

    load_data(csv_data)
    load_data(json_data)
    load_data(api_data)

    log_message("ETL Job Completed.\n")
```

### 7. Running the Script

Execute the script using Python:

```bash
python etl_pipeline.py
```

This will create or update `database.db` and `etl_log.txt` files in the project folder.

---

## Notes

- Make sure the `data/` folder contains the sample CSV and JSON files.
- Internet connection is required for the API extraction.
- Logs are appended; check `etl_log.txt` for details on each run.
- If the database or log files do not appear, ensure you are running the script from the project root directory.

---

---

## ðŸ”¥ Automating the ETL Pipeline on Windows Using Task Scheduler

If you're on Windows and want to schedule your ETL script to run automatically (like a cron job on Linux/macOS), Task Scheduler is your go-to tool.

Here's a step-by-step guide so *anyone* can set this up on their laptop:

### How to Schedule Your ETL Script with Task Scheduler

1. **Open Task Scheduler**  
   - Press `Win + S` to open the Windows Search bar.  
   - Type **Task Scheduler** and hit Enter or click the app to open it.

2. **Create a New Basic Task**  
   - In the **Actions** panel on the right side, click **Create Basic Task...**  
   - Give your task a name, e.g., `ETL Automation Job`  
   - Optionally add a description, like:  
     *Runs the ETL pipeline automatically on schedule.*

3. **Set the Trigger (When to Run)**  
   - Choose when you want the script to run, e.g., **Daily**, **Weekly**, or **One time** (for testing).  
   - Set the exact time and recurrence pattern as desired.

4. **Configure the Action (What to Run)**  
   - Select **Start a program** and click **Next**.

5. **Specify the Program and Script**  
   - In the **Program/script** box, enter the full path to your Python executable, for example:  
     ```
     C:\Program Files\Python312\python.exe
     ```  
   - In the **Add arguments (optional)** box, enter the script filename, e.g.:  
     ```
     etl_pipeline.py
     ```  
   - In the **Start in (optional)** box, enter the folder path where your script lives, e.g.:  
     ```
     C:\Users\Lenovo\Engects\DataScience\etl_project
     ```

6. **Finish**  
   - Review your settings and click **Finish**.

---

### Tips:
- Make sure your script and all data files are located where you set the "Start in" path.
- If your script requires internet or special permissions, run Task Scheduler as Administrator.
- You can always edit or delete the task later from Task Scheduler's **Task Scheduler Library**.

This setup allows your ETL pipeline to run hands-free on your Windows machine, keeping your data up-to-date automatically!

---

## License

MIT License

---

## Author

Charan Teja B S [https://github.com/CharanTeja-BS/]
